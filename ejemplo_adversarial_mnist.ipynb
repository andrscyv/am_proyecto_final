{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de un ejemplo adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      " - 40s - loss: 0.1986 - categorical_accuracy: 0.9388 - val_loss: 0.0495 - val_categorical_accuracy: 0.9835\n",
      "Epoch 2/3\n",
      " - 39s - loss: 0.0756 - categorical_accuracy: 0.9772 - val_loss: 0.0372 - val_categorical_accuracy: 0.9873\n",
      "Epoch 3/3\n",
      " - 51s - loss: 0.0561 - categorical_accuracy: 0.9830 - val_loss: 0.0332 - val_categorical_accuracy: 0.9891\n",
      "60000/60000 [==============================] - 18s 306us/step\n",
      "[0.01913473764181981, 0.9941499829292297]\n",
      "10000/10000 [==============================] - 3s 273us/step\n",
      "[0.03318603273532863, 0.9890999794006348]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten ,Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape, Add\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.regularizers import l1_l2, l2, l1\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras.preprocessing.image import array_to_img,img_to_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#preprocess data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.\n",
    "x_test /= 255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "#compile CNN network for MNIST classification \n",
    "inputs = Input(shape=(28,28,1))\n",
    "net = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(inputs)\n",
    "net = Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu')(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = Dropout(0.25)(net)\n",
    "net = Flatten()(net)\n",
    "net = Dense(128, activation='relu')(net)\n",
    "net = Dropout(0.5)(net)\n",
    "outputs = Dense(10, activation='softmax')(net)\n",
    "\n",
    "mnist_model = Model(inputs=inputs, outputs=outputs, name='classification_model')\n",
    "mnist_model.compile(optimizer='nadam', loss='categorical_crossentropy',metrics=[categorical_accuracy])\n",
    "\n",
    "#train MNIST classifer\n",
    "earlyStop = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "                          baseline=None, restore_best_weights=True)\n",
    "\n",
    "#mnist_model.fit(x_train, y_train, batch_size=128, epochs=100, verbose=0, validation_data=(x_test, y_test),\n",
    "#               callbacks=[earlyStop])\n",
    "mnist_model.fit(x_train, y_train, batch_size=128, epochs=3, verbose=2, validation_data=(x_test, y_test))\n",
    "\n",
    "print(mnist_model.evaluate(x_train, y_train))\n",
    "print(mnist_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eligamos una imagen para crear ejemplo adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select image to create an adversarial example from\n",
    "img = x_train[0:1]\n",
    "plt.imshow(img.reshape((28,28)),vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué clasificacion da el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7737483e-11 6.2608922e-09 8.9847067e-11 3.6302149e-02 5.0390000e-12\n",
      " 9.6368796e-01 3.2505997e-07 7.6546360e-09 1.2309062e-06 8.2603947e-06]\n"
     ]
    }
   ],
   "source": [
    "#varify accurate classificaiton\n",
    "prediction = mnist_model.predict(img)[0]\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAYXUlEQVR4nO3de3TU5ZkH8O8zkxsJSbgEMNwDBAFRQYOg0uIVkW3Ftqes2FrLWrBVe7R6bK3tttqtu55ase5arXRVsN6WrVU4FW2VVZRqKUHuFwn3W0git0BISDLz7B8ZamrzPr84k8yMfb+fcziEfOedeRnyMJM8v/d9RVVBRP/4QqmeABElB4udyBMsdiJPsNiJPMFiJ/JERjIfLEuyNQd5cY+PdnOPDdc3BQwO6DqE7f/3tOGkPd4gOdkBNxD7sesb7OHhsHtsJGLfd2GumTd2N2Nk7zxh38DQ3Mv+WogGfHVmVdaZuXTJcWZBz2ljsT23oMdOlQbUoVFPtvkFlVCxi8gUAA8DCAP4b1W937p9DvIwXi6N+/FOXDLemeWvrzHHSr1drJpvf9FHNlWYuSU8dLj92Fn2P0N09Ub7/gsKnVnkyFFz7MmJ48x89/SomZfOfN/MYbR2q//5AnNoQ5F91wPvfdfMQ8NHOLPo2s3m2D032HMbcJ/92KmyXJc4s7jfxotIGMAvAVwJYBSAGSIyKt77I6LOlcj37OcB2Kqq21W1EcALAKZ1zLSIqKMlUuz9AOxp9ee9sc/9DRGZLSLlIlLehPi/7yWixHT6T+NVda6qlqlqWSYCflBFRJ0mkWLfB2BAqz/3j32OiNJQIsW+AkCpiJSISBaAawAs6phpEVFHi7v1pqrNInILgD+gpfX2pKpusMaICEI57t7ngW+cYz5m70fc7Q67m5y4w18/35mFmuwe/pHT7T764Pvs9lXTZeeaOd5YaeeG7MUrzDxvtN2CslprABDu7m7U59bYbb2+i/aaebOZAgfPMR67n91yHPia3bIMGX8vANj7LyPNvGiN++dXmmG/Bme9Zv+buSTUZ1fVxQAWJ3IfRJQcvFyWyBMsdiJPsNiJPMFiJ/IEi53IEyx2Ik9IMneXzS/sr+dc8G1nnvtBtTleM41O4YeHzbGRw3Ye7uZeJgoELxX9RxUePtS+QdTulX/tlbec2bxrp5pjdaV52UagLkv7OLP6SVXmWMm2L+2uusG+9qH3o/YS2Oqb3Ncv9Fpj7xGwf6J7OfaOeXNQX7mnzQs7+MpO5AkWO5EnWOxEnmCxE3mCxU7kCRY7kSeS2norzO2rE0bMcuabb7a37x3yvLvNk3G80Rx7op+9e2zuS8vN3BLKzzfzHXeONvOSBXZbMLre3gk1EaFc+3mJnoh/q2gAiFzsXractdFewhqpsluxMvYMM28odv/dso7YXy/y7hozT5i1fXgCNblcl6BWD7H1RuQzFjuRJ1jsRJ5gsRN5gsVO5AkWO5EnWOxEnkjqkc0nu4Wx6/PdnPnwWfaywG0PTnBmQ++wt1NumOneChoA7G4zIOe6e7pN+fZyyHC9vZX04THu5wQACgrOtu//hPu46qATYKNnDTPzjD0fmnnzvv1mHn7TvU12JOQ+aro9dnyxwMyHPVXpfuye9rURGjC35ovHmHl9r0wzz3/hz84s3KuXObZpVH93WO6uIb6yE3mCxU7kCRY7kSdY7ESeYLETeYLFTuQJFjuRJ5LaZ8+uqsegOe51wvamxInp+Rv7mNuaG+0+/JHT3WuMh93u7pkCQMbZ9rHHhc/Y43f+1J7b4B++Z+aWHbfa1wCUzLD76BWPjDfzz01w99mXvGgfm7zi5l+Y+YX328/rK8tedmYXrb/aHNs71772YcGQJ8z8ir52H97corvavrYhtHSVO9R6Z5RQsYvITgDH0HI8erOqliVyf0TUeTrilf1iVbX/KyKilOP37ESeSLTYFcAfRWSliMxu6wYiMltEykWkvFEbEnw4IopXom/jJ6rqPhHpDeB1Edmsqm+3voGqzgUwFwAKw0XJ292SiP5GQq/sqrov9ns1gJcAnNcRkyKijhd3sYtInojkn/oYwGQA6ztqYkTUsRJ5G98HwEvSsv91BoDnVPU1a4BGo4jW1cX9gMMfr3FmkYCxkQvPNPOix+1edVHA/Vv6zbePHq74md1H7/W+/d3Pvdvda/kn5AStGV9tx3abHcej9jUCvzzsft43fPtR+86RZaZHR9j/6sOe/ZYz2/qVx8yxpz/lHgsAGLLEzgNc9bL7eVs447Pm2IqfjHBmJ3/mvt+4i11VtwOwd1UgorTB1huRJ1jsRJ5gsRN5gsVO5AkWO5Enknpkc0Gop07InOLMo+NGmuOHPfyBM9s2zr4UN2h73kiNu60XJFxgb2m8ePPbZj5z92fM/KmB73ziOZ3y7x+ebuZ3F7mf0/a4rdJe6Dgw+5Az+1rhOnPsdaWXmvnxK+1mUNV492tZ3l57ae+RsfaRzsNvKDfzIPXT3Nef5b7iXhYMANrc7Mx4ZDMRsdiJfMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8kRSt5KWUAihrnnOvKGbvaRxy3dHObMw7N5kIn10AIh+Zqz7sSuPmGPn1fY286A++qh3v2rmr4z7lTOb2NXuo49dcY2Z9ys8auaVx+yjj1ee6+5HP107yBwbbbCvnch/a4uZ57502JlV32JvQx3URw93KzTzyBH7eeuy8C/OLHq+ff3AnivcNdT4uHuJK1/ZiTzBYifyBIudyBMsdiJPsNiJPMFiJ/IEi53IE0ntsyMchnRzr/3OXmwfqxzKyXFmQcc9Rye5++QAkHHYfdQtAOy+pIszG/ySPfY/f/ElM//9VyrM/Bsj3jXzksyuRmYOxRm9Dpj5wUvsXndRQ6WZn3nnTc5s3XfsraSfRX8zbxxTYuZZ+3s6s96P2M9pkKA++t7v2338QY+5txePvOc+1hwAiordx2TvO+Eex1d2Ik+w2Ik8wWIn8gSLncgTLHYiT7DYiTzBYifyRFL77NrYiOYdu+IeH7S+2VJfZK+VP3iRu48OAANfPebMoms3m2N7rTVj1P3ZvU4fAH41vdTMHzNOZb78klXm2F8MeMXMrwtdbubb77ePmx72jHvf+CseGGOOzRgy2MwPnOW+7gIATnvTvcdBuHt3c6zk2l8Pzfvss6z7/0dAH790iDsL6OE35rlfo9X4Wgh8ZReRJ0WkWkTWt/pcDxF5XUQqYr/bzxwRpVx73sbPA/DxY1zuArBEVUsBLIn9mYjSWGCxq+rbAD7+XmwagPmxj+cDuLqD50VEHSze79n7qOqpi6IPAOjjuqGIzAYwGwBykBvnwxFRohL+aby2nAzpPB1SVeeqapmqlmUiO9GHI6I4xVvsVSJSDACx36s7bkpE1BniLfZFAK6PfXw9gIUdMx0i6iyB57OLyPMALgJQBKAKwI8BvAxgAYCBAHYBmK6q7oZqTIH00PHiPnP7+PQJ5vj9k93nUkuD0WAEkHPAzgf8NLH1zal05Dp3r/vzd75pjn11v93j7zple1xz6gj1V7vPMAeArhV2Pzqywb1n/tbf2PsbDLvOvj6hM219yK6DYd9x7w1vnc8e+AM6VZ3hiNxVS0Rph5fLEnmCxU7kCRY7kSdY7ESeYLETeSKw9daRglpvQcJF7q2Bd954ujm2ZN5OMw9asljzTXd7q9ev3jPHJqrmW/Yy0m7bmpxZU1e75fjOI4+b+T9dOM3ME1myDGmzQ/RRXDbazHXFOjOvvtm9nXPvX356W63773T/vbbPn4P6A3vafGL5yk7kCRY7kSdY7ESeYLETeYLFTuQJFjuRJ1jsRJ5I7pHNAZouO9e+wRsrnVHPTRFzaPOAIvu+K6vMuLN76eZjP2Y/tl5wtjOrOdveQnvB8UIzr5jV18wnXfqhme8eX+fMPpxlL+Usmmv/vWXcmWbe/YOTZm6JTrS3uT5Sam81nVftXo4NANmv2MeTW/o+4L5GYI+6n2++shN5gsVO5AkWO5EnWOxEnmCxE3mCxU7kCRY7kSc+VevZmy919+Ezlrh78EBw3zS0bHVccwKAC9Y0mvniByeZ+dFh9v0P+lHqevx/2G8/L0N+e6OZf+4C97HJdc32CUG7v2cfVR1a2nnbPWf072fmevy4mTeNLjFz6+tt173u9eoAMOjH7j67tZU0X9mJPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8gSLncgTadVnr73WXt9c8Jz7qNqMfva6ay3IM/PIpgozt1Q8bM+7+E/2+K4L3H8vAAidNcLMG3u5/25N+fa+8YO/t9nMq86vNfMZm+399l876N77/YWS/zPHBhn5+E1mPvDe9N0b3toTP1Rnr8OvHdXDma1d8jCOH4pz33gReVJEqkVkfavP3SMi+0RkdezX1KD7IaLUas/b+HkAprTx+YdUdUzs1+KOnRYRdbTAYlfVtwEcSsJciKgTJfIDultEZG3sbX53141EZLaIlItIeRPi3xOMiBITb7E/BmAogDEAKgE86Lqhqs5V1TJVLcuEvfCBiDpPXMWuqlWqGlHVKIBfAzivY6dFRB0trmIXkeJWf/wCgPWu2xJRegjcN15EngdwEYAiEdkL4McALhKRMQAUwE4A9qLmduq+6qCZWzvDa2FXe+zGLXHM6CMZxac5s9Jb7T757h/Z65Mzp44z8+zF9h7jmWPPcGb1Rfnm2KA+et2Xxpv5/1xzLOD+jX3p/9Xus+9utteMD154xMyjZtq59n/X/jfv+4B7j4JIwLUvBU3utfLhE03OLLDYVXVGG59+ImgcEaUXXi5L5AkWO5EnWOxEnmCxE3mCxU7kibRa4hrk5JXuFlX2q/EfgZsoPd99ZDIAyHtrzDzcq5eZR2pqPvGcTolOGmvmWVsqzby58kDcjx0kaJvqC9d+0cz/dNbv7PG3fdOZBS0rHroix8y3jWsw8yDW8eSZxtHkQbiVNBGx2Il8wWIn8gSLncgTLHYiT7DYiTzBYifyROCqt44U7ZaL+ovc+1zsndxme/AjxhrX0lftoce/bC/V7Pq/y+07MAT10YME9dFDublmfvSqs5zZwdH2czp4qd1H33t3wPHBv60y88VvvejMnq4tMsfeVLLUzEcsu87MBwX00i0lXex/kx359tblEg54HU2glx4uHeJ+3F3vODO+shN5gsVO5AkWO5EnWOxEnmCxE3mCxU7kCRY7kSfSaj17l6V9zPH1k+yebkLOO9OMNcP9/+KxQV3MsQXP2/1eOde9FTQAhGrrzTxSsd1938bRwABQObHAzNd891EzT8SWpjozH55pH7P94nF77nOHu/vRqRbu7jwxDVpv/3tLjvtkpfdqF+Jocw3XsxP5jMVO5AkWO5EnWOxEnmCxE3mCxU7kCRY7kSeSup49yMmp9hG9obNHOrPomk3m2HCB3ZON/GWdmVurwgveNYcGHtk88Cf2HWxfYF8D0Nzk3ht+68VPmWODHI3aPd/pH3zZzL/U931n9vPVl5tjh8yxD13ePdk+jnpQnnufASnubY6NbN1h5omSru5rCCKHD9uDG9x71qu6N30IfGUXkQEi8qaIbBSRDSJya+zzPUTkdRGpiP3uvkqAiFKuPW/jmwHcoaqjAEwAcLOIjAJwF4AlqloKYEnsz0SUpgKLXVUrVfX92MfHAGwC0A/ANADzYzebD+DqzpokESXuE33PLiKDAYwFsBxAH1U9dVDYAQBtXtguIrMBzAaAHNh7qRFR52n3T+NFpCuAFwHcpqq1rTNtWU3T5ooaVZ2rqmWqWpYJ9wX8RNS52lXsIpKJlkJ/VlVPHZ1ZJSLFsbwYQHXnTJGIOkLg23gREQBPANikqnNaRYsAXA/g/tjvCwPvKzsLGf0HO/OGwT3N8dmr3Es5g0TrEztid+e/ne/MCrfZY3uvajbzI19z3zcAYJO9HfTWWfEvQ714wzQzf7T0eTP/w8jfm/mUq77qzErK7S24991ptywH3Ge3LK3GXUb9SXNsoiTbfhe769qBzizjhDsDgD7/FdDrdd1vO25zIYDrAKwTkVMHat+NliJfICI3ANgFYHpcMyCipAgsdlVdBvc1Je6dKIgorfByWSJPsNiJPMFiJ/IEi53IEyx2Ik8kdYlrQ59MbLrDvV30gFftba0Tuf5OmxoTGA30/ZO7V36sv/00Ln18bkKPfVXFlITGW75fstjMZ/7wdjMvfCboWOT1zqRxyjhzZN8H7H5y0+QyM89ettGZRbvbS553PWRf8zFour0kOjR0kJnXlbq/HkfOqXVmANB84Rh3uNr9nPGVncgTLHYiT7DYiTzBYifyBIudyBMsdiJPsNiJPJFWRzZHJ7m3RAaA0NJVcT92Rond96y8oq+Zz7vrIWeWL/Z69f4Z9pHOM3fZiwefGfyWmc+rdW+LfFmuvQfArBGTzTx0mr3lstYeM/ODVw53ZjmH3dseA8DJbvZrUc4he3zehgPOrHnXHnPsp9VyXYJaPcQjm4l8xmIn8gSLncgTLHYiT7DYiTzBYifyBIudyBNpdWRzIn30mm/ae69Hphwx8+fGzDHzM7LcvfJFdfaxVsf0kJnXXGDP7fT7vmXmg3/wnjN7eOEl5tjeJzabuRw+auZZL2eZebdJ7rkFyYl7ZAststekf1qFhw91ZrJzmTPjKzuRJ1jsRJ5gsRN5gsVO5AkWO5EnWOxEnmCxE3miPeezDwDwNIA+ABTAXFV9WETuATALQE3spnerqr0JeSdqLLTPMO/3BbuffLvafXoZd6Y7O9lkjo3k2TveC+xzyq0+OgBU3u4+x/y0++vMsUHr/Jt37DLz+klmjPDIUmcW2VRhjpUM+8tTm+19BCIfHjTzRNReO8HMNeBltMeqw84smmX/vev65bnHHgg7s/ZcVNMM4A5VfV9E8gGsFJHXY9lDqvrzdtwHEaVYe85nrwRQGfv4mIhsAtCvsydGRB3rE33PLiKDAYwFsDz2qVtEZK2IPCki3R1jZotIuYiUN+FkQpMlovi1u9hFpCuAFwHcpqq1AB4DMBTAGLS88j/Y1jhVnauqZapalpnQaW1ElIh2FbuIZKKl0J9V1d8BgKpWqWpEVaMAfg3gvM6bJhElKrDYRUQAPAFgk6rOafX54lY3+wKs4zqJKOUCt5IWkYkA3gGwDkA09um7AcxAy1t4BbATwI2xH+Y5FXYp1vOHzHTmQa2Y8Cj3tsSRjVvMsaExo8xc9tWYeaTGzhNRP81+U9Rl4V/M3FryGN1ut86OTrePPS54LuBI5pC71dMyAXu754SI3W5FAtukS9lo+67L7de26pvc7VAAqOvvnlvJ3Xardc5Od37N56qxYW1jm09Me34avwxAW4NT1lMnok+OV9AReYLFTuQJFjuRJ1jsRJ5gsRN5gsVO5ImkbiWtDScDe+mWQ2N7OLPCjfbY6Gr7Bk2XnWvmmW+4++zbnhtjjh167WozP9bP/mewD3wG6oe4n5esLdvMsYF99CAJ9NGDetG9H33XvoOAPvqhme5lyz2esnvZQX30IIFzT8Cd465yZnsP/daZ8ZWdyBMsdiJPsNiJPMFiJ/IEi53IEyx2Ik+w2Ik8EbievUMfTKQGQOsF1kUAPkzaBD6ZdJ1bus4L4Nzi1ZFzG6SqvdoKklrsf/fgIuWqau+ekCLpOrd0nRfAucUrWXPj23giT7DYiTyR6mKfm+LHt6Tr3NJ1XgDnFq+kzC2l37MTUfKk+pWdiJKExU7kiZQUu4hMEZEPRGSriNyVijm4iMhOEVknIqtFpDzFc3lSRKpFZH2rz/UQkddFpCL2e5tn7KVobveIyL7Yc7daRKamaG4DRORNEdkoIhtE5NbY51P63BnzSsrzlvTv2UUkDGALgMsB7AWwAsAMVQ3YfiI5RGQngDJVTfkFGCLyWQDHATytqqNjn/sZgEOqen/sP8ruqvq9NJnbPQCOp/oY79hpRcWtjxkHcDWAryOFz50xr+lIwvOWilf28wBsVdXtqtoI4AUA01Iwj7Snqm8DOPSxT08DMD/28Xy0fLEknWNuaUFVK1X1/djHxwCcOmY8pc+dMa+kSEWx9wOwp9Wf9yK9zntXAH8UkZUiMjvVk2lDn1bHbB0A0CeVk2lD4DHeyfSxY8bT5rmL5/jzRPEHdH9voqqeA+BKADfH3q6mJW35HiydeqftOsY7Wdo4ZvyvUvncxXv8eaJSUez7AAxo9ef+sc+lBVXdF/u9GsBLSL+jqKtOnaAb+706xfP5q3Q6xrutY8aRBs9dKo8/T0WxrwBQKiIlIpIF4BoAi1Iwj78jInmxH5xARPIATEb6HUW9CMD1sY+vB7AwhXP5G+lyjLfrmHGk+LlL+fHnqpr0XwCmouUn8tsA/CAVc3DMawiANbFfG1I9NwDPo+VtXRNafrZxA4CeAJYAqADwBoAeaTS336DlaO+1aCms4hTNbSJa3qKvBbA69mtqqp87Y15Jed54uSyRJ/gDOiJPsNiJPMFiJ/IEi53IEyx2Ik+w2Ik8wWIn8sT/A0rXdRdAHLGlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1083179e-06 8.7270295e-05 2.7041782e-05 3.9616412e-01 1.4128898e-06\n",
      " 5.9841907e-01 4.0781495e-04 2.2897200e-04 2.1130473e-03 2.5491668e-03]\n"
     ]
    }
   ],
   "source": [
    "#applying random noise does not fool the classifier\n",
    "quantized_noise = np.round(np.random.normal(loc=0.0, scale=0.3, size=img.shape) * 255.) / 255.\n",
    "noisy_img = np.clip(img + quantized_noise, 0., 1.)\n",
    "plt.imshow(noisy_img.reshape((28,28)),vmin=0., vmax=1.)\n",
    "plt.show()\n",
    "noisy_prediction = mnist_model.predict(noisy_img)[0]\n",
    "print(noisy_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.3112 - categorical_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.6810 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2750 - categorical_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2528 - categorical_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2203 - categorical_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.2093 - categorical_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1604 - categorical_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.7124 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.0707 - categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2396 - categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -2.5727 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2185 - categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1501 - categorical_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.4267 - categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.1361 - categorical_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -0.0107 - categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0948 - categorical_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.2263 - categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1307 - categorical_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -0.2913 - categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1927 - categorical_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.9270 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1617 - categorical_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1597 - categorical_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0101 - categorical_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -1.0502 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -1.5541 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.1281 - categorical_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0467 - categorical_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -0.4957 - categorical_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -1.3460 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.7932 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.1146 - categorical_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -1.0754 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.4229 - categorical_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.0407 - categorical_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.1379 - categorical_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -1.0531 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -0.6374 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -1.2620 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0525 - categorical_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -2.3105 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -1.8008 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -3.3881 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -2.1610 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -1.2963 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -1.9879 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -1.0107 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -4.0854 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -8.9970 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -4.4106 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -3.1383 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -1.5680 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -2.0616 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -4.6111 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -4.9422 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -6.0418 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -2.1526 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -9.7167 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -9.4754 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -5.5711 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -7.5054 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -8.8681 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -2.2157 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -6.1064 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -10.9652 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -5.1056 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -6.8232 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -5.8918 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -5.4635 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -10.5885 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -3.4426 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -8.8309 - categorical_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -5.7292 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -3.3240 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -10.8695 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -7.9935 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -15.6881 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -12.5343 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -16.6466 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -17.2384 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -15.4596 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -9.0697 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -3.9553 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -10.6464 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -13.5346 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -6.2279 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -9.1796 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -8.1307 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -18.7894 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -14.1374 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -4.5061 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -5.5085 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -14.4817 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -13.1010 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -13.3289 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -11.1054 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: -11.0430 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -18.4073 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: -13.9076 - categorical_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = './adversarial_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-425e49882510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madversarial_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mgenerate_adversary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mgenerate_adversary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mgenerate_adversary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-425e49882510>\u001b[0m in \u001b[0;36mgenerate_adversary\u001b[0;34m(img, target, model, regularization, loss_function)\u001b[0m\n\u001b[1;32m     52\u001b[0m                          \u001b[0;31m#callbacks=[checkpoint])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#restore best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0madversarial_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./adversarial_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#quantize adversarial noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = './adversarial_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "#custom activation function for keeping adversarial pixel values between 0.0 and 1.0\n",
    "def clip(x):\n",
    "    return K.clip(x, 0.0, 1.0)\n",
    "\n",
    "#custom loss funciton for non-targeted misclassification\n",
    "def negative_categorical_crossentropy(yTrue,yPred):\n",
    "    return 0.0 - K.categorical_crossentropy(yTrue,yPred)\n",
    "\n",
    "#add custom objects to dictionary\n",
    "get_custom_objects().update({'clip': Activation(clip)})\n",
    "get_custom_objects().update({'negative_categorical_crossentropy': negative_categorical_crossentropy})\n",
    "\n",
    "\n",
    "#function for generating an adversarial example given a base image, adversarial class target, classifier, and regularization type\n",
    "def generate_adversary(img,target,model,regularization,loss_function):\n",
    "    \n",
    "    #input for base image\n",
    "    image = Input(shape=(28,28,1),name='image')\n",
    "    #unit input for adversarial noise\n",
    "    one = Input(shape=(1,),name='unity')\n",
    "    \n",
    "    #layer for learning adversarial noise to apply to image\n",
    "    noise = Dense(28*28,activation = None,use_bias=False,kernel_initializer='random_normal',\n",
    "                  kernel_regularizer=regularization, name='adversarial_noise')(one)\n",
    "    \n",
    "    #reshape noise in shape of image\n",
    "    noise = Reshape((28,28,1),name='reshape')(noise)\n",
    "    \n",
    "    #add noise to image\n",
    "    net = Add(name='add')([noise,image])\n",
    "    #clip values to be within 0.0 and 1.0\n",
    "    net = Activation('clip',name='clip_values')(net)\n",
    "    \n",
    "    #feed adversarial image to trained MNIST classifier\n",
    "    outputs = model(net)\n",
    "\n",
    "    adversarial_model = Model(inputs=[image,one], outputs=outputs)\n",
    "    #freeze trained MNIST classifier layers\n",
    "    adversarial_model.layers[-1].trainable = False\n",
    "    \n",
    "    adversarial_model.compile(optimizer='nadam', loss=loss_function, metrics=[categorical_accuracy])\n",
    "        \n",
    "    #target adversarial classification\n",
    "    target_vector = np.zeros(10)\n",
    "    target_vector[target] = 1.\n",
    "    \n",
    "    #callback for saving weights with smallest loss\n",
    "    checkpoint = ModelCheckpoint('./adversarial_weights.h5', monitor='loss', verbose=0, save_best_only=True, save_weights_only=True,\n",
    "                                 mode='auto', period=1)\n",
    "    #train adversarial image\n",
    "    adversarial_model.fit(x={'image':img,'unity':np.ones(shape=(1,1))},y=target_vector.reshape(1,-1),epochs=100,verbose=1)\n",
    "                         #callbacks=[checkpoint])\n",
    "    #restore best weights\n",
    "    adversarial_model.load_weights('./adversarial_weights.h5')\n",
    "    \n",
    "    #quantize adversarial noise\n",
    "    quantized_weights = np.round(adversarial_model.get_weights()[0].reshape((28,28)) * 255.) / 255.\n",
    "    \n",
    "    #add trained weights to original image and clip values to produce adversarial image\n",
    "    adversarial_img = np.clip(img.reshape((28,28)) + quantized_weights, 0., 1.)\n",
    "    \n",
    "    #display adversarial image\n",
    "    plt.imshow(adversarial_img,vmin=0., vmax=1.)\n",
    "    plt.show()\n",
    "    #classify adversarial image\n",
    "    adversarial_prediction = mnist_model.predict(adversarial_img.reshape((1,28,28,1)))\n",
    "    print(adversarial_prediction)\n",
    "    \n",
    "    return adversarial_img\n",
    "\n",
    "generate_adversary(img,5,mnist_model,l1(0.01),'negative_categorical_crossentropy')\n",
    "generate_adversary(img,5,mnist_model,l2(0.01),'negative_categorical_crossentropy')\n",
    "generate_adversary(img,5,mnist_model,l1_l2(l1=0.01,l2=0.01),'negative_categorical_crossentropy')\n",
    "\n",
    "generate_adversary(img,9,mnist_model,l1(0.01),'categorical_crossentropy')\n",
    "generate_adversary(img,9,mnist_model,l2(0.01),'categorical_crossentropy')\n",
    "generate_adversary(img,9,mnist_model,l1_l2(l1=0.01,l2=0.01),'categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
